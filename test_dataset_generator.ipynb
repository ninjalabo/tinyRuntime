{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb3df77",
   "metadata": {},
   "source": [
    "# Test dataset generator\n",
    "We will prepare tensor binary files from imagenette (valid) dataset for C runtime here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from torch.utils.data import Dataset\n",
    "import struct\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8face",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE_320,data=Path.cwd()/'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842e7e9",
   "metadata": {},
   "source": [
    "We will eventually generate raw tensor binary files for C runtime as a `test` dataset. Those files have a file extension `.bin`. The name of parent direcotries have been already encoded from `0` to `9` accordingly. So we need to modify `ImageBlock` and `get_y` to support a newly generated `test` dataset. And we set up that before `test` dataset is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a133f6",
   "metadata": {},
   "source": [
    "Usually `ImageBlock` takes care of most of image file conversion but in our case eventually we'll get a raw tensor binary files for a test dataset. To deal all of them in an unified way, we need to implement our file loader below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07022d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTensorLoader(Transform):\n",
    "    def encodes(self, fn:Path):\n",
    "        fn = str(fn)\n",
    "        if fn.lower().endswith('.jpg') or fn.lower().endswith('.jpeg'):\n",
    "            return PILImage.create(fn)\n",
    "        elif fn.lower().endswith('.bin'):\n",
    "            with open(fn, 'rb') as f:\n",
    "                x = struct.unpack(f'{3*224*224}f', f.read())\n",
    "            x = torch.tensor(x).view(3, 224, 224)\n",
    "            mean, std = [torch.tensor(o).view(3,1,1) for o in imagenet_stats]\n",
    "            x = x * std + mean\n",
    "            return PILImage.create(to_pil_image(x))\n",
    "        else:\n",
    "            raise Exception(f'Unknown file type for {fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a026b1e",
   "metadata": {},
   "source": [
    "Not only with a custom file loader, we use encoded `label`s for raw tensor binary files. If the `label` is numerical value from `0` to `9`, they should be decoded back to the original `label` to align with the original jpeg datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = L(o for o in get_files(path) if str(o).lower().endswith('jpeg') or str(o).lower().endswith('jpg'))\n",
    "vocab = list(set(o.parent.name for o in x))\n",
    "i2o = {i:o for i, o in enumerate(vocab)}\n",
    "\n",
    "def my_parent_label(fn:Path):\n",
    "    pa = parent_label(fn)\n",
    "    return i2o[int(pa)] if pa.isdigit() else pa    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa315691",
   "metadata": {},
   "source": [
    "Here, we'll build up our own dataloader to handle the original jpeg files and raw tensor binary files at once, although we still haven't generated such raw tensor binary files yet. The original jpeg files for either `train` or `valid` dataset and newly generated raw tensor binary files for `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49056046",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBlock(\n",
    "    blocks=(TransformBlock(type_tfms=ImageTensorLoader), CategoryBlock),\n",
    "    get_items=get_files,\n",
    "    splitter=GrandparentSplitter(valid_name='val'),\n",
    "    get_y=my_parent_label,\n",
    "    item_tfms=Resize(224)\n",
    ")\n",
    "dls = db.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39930f6",
   "metadata": {},
   "source": [
    "If model parameters were saved in a file previously, load those previously trained parameters. Otherwise, run `finetune` and save those newly trained parameters into a file for the future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b70b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = path/'model.pth'\n",
    "if os.path.exists(fn):\n",
    "    learn.model.load_state_dict(torch.load(fn))\n",
    "else:\n",
    "    learn.fine_tune(1)\n",
    "    torch.save(learn.model.state_dict(), fn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a86468",
   "metadata": {},
   "source": [
    "We'll generate `test` dataset while running inference for `validation` dataset. `test` dataset is augmented from `valid` dataset and stored in `tensor` format for C runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveImageFilesCallback(Callback):\n",
    "    def __init__(self, save_dir, ncat=10, nitems=100):\n",
    "        self.save_dir = save_dir\n",
    "        self.ncat = ncat\n",
    "        self.counts = [0] * int(nitems/ncat)\n",
    "        for i in range(self.ncat):\n",
    "            os.makedirs(save_dir/str(i), exist_ok=True)\n",
    "   \n",
    "    def after_batch(self):\n",
    "        if self.learn.training or self.learn.model.training:\n",
    "            return\n",
    "        for X,y in zip(self.learn.xb[0], self.learn.y):\n",
    "            if self.counts[y] >= self.ncat:\n",
    "                continue\n",
    "            \n",
    "            fn = f'{str(self.save_dir)}/{y.item()}/{self.counts[y]}'\n",
    "            with open(fn+'.bin', \"wb\") as f:\n",
    "                f.write(struct.pack('f'*X.numel(), *X.flatten()))                \n",
    "            \n",
    "            self.counts[y] += 1\n",
    "            if sum(self.counts)==self.ncat*len(self.dls.vocab):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75aff84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min, sys: 1min 45s, total: 8min 45s\n",
      "Wall time: 1min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#2) [337.6174621582031,0.10140127688646317]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learn.validate(cbs=SaveImageFilesCallback(path/'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3ca15",
   "metadata": {},
   "source": [
    "## Test dataset directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd08fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;5mdata/imagenette2-320/test\u001b[0m\n",
      "├── \u001b[38;5;5m0\u001b[0m\n",
      "├── \u001b[38;5;5m1\u001b[0m\n",
      "├── \u001b[38;5;5m2\u001b[0m\n",
      "├── \u001b[38;5;5m3\u001b[0m\n",
      "├── \u001b[38;5;5m4\u001b[0m\n",
      "├── \u001b[38;5;5m5\u001b[0m\n",
      "├── \u001b[38;5;5m6\u001b[0m\n",
      "├── \u001b[38;5;5m7\u001b[0m\n",
      "├── \u001b[38;5;5m8\u001b[0m\n",
      "└── \u001b[38;5;5m9\u001b[0m\n",
      "\n",
      "10 directories\n",
      "-rw-rw-r-- 1 doyu doyu 602112 Apr 23 11:37 data//imagenette2-320/test/2/3.bin\n",
      "-rw-rw-r-- 1 doyu doyu 602112 Apr 23 11:37 data//imagenette2-320/test/2/7.bin\n",
      "-rw-rw-r-- 1 doyu doyu 602112 Apr 23 11:37 data//imagenette2-320/test/6/3.bin\n",
      "-rw-rw-r-- 1 doyu doyu 602112 Apr 23 11:37 data//imagenette2-320/test/6/7.bin\n"
     ]
    }
   ],
   "source": [
    "!tree -d data/imagenette2-320/test\n",
    "!ls -al data//imagenette2-320/test/[2,6]/[3,7].bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9201a76",
   "metadata": {},
   "source": [
    "save `learner` to use later, and then, load `learner` & run inference for `test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c421dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()\n",
    "learn = load_learner(path/'export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d92ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = dls.test_dl(get_files(path, extensions=['.bin']), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9675ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [382.3768310546875,0.10000000149011612]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(dl=test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
