{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import platform\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from export import export_model, Quantizer, export_model_sq8\n",
    "from train import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630825db",
   "metadata": {},
   "source": [
    "**NOTE** 26.9.2024: Currently this notebook runs 3 inferences: PyTorch, vanilla tinyRuntime and statically quantized tinyRuntime. If dynamically quantized or some other configuration of tinyRuntime performs better than current quantized tinyRuntime, please update this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f57980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_c(dir_path, base_command, model_path):\n",
    "    '''Run C inference and return dictionary with accuracy, duration, model size, and memory usage.'''\n",
    "    # get file paths of images and their labels\n",
    "    files = []\n",
    "    for label in range(10):\n",
    "        sd_path = os.path.join(dir_path, str(label))\n",
    "        f_paths = [os.path.join(sd_path, file) for file in os.listdir(sd_path)]\n",
    "        files += f_paths\n",
    "\n",
    "    # run C inference\n",
    "    command = [base_command, model_path, *files]\n",
    "    mems = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    proc = psutil.Process(process.pid)\n",
    "    while process.poll() == None:\n",
    "        try:\n",
    "            # `proc.memory_info().rss` returns the physical memory the process has used\n",
    "            mems.append(proc.memory_info().rss / (1024 * 1024)) # append in megabytes\n",
    "            time.sleep(0.1)  # check memory usage every 0.1 second\n",
    "        except psutil.NoSuchProcess: # handle the case where the process ends abruptly\n",
    "            pass\n",
    "    end_time = time.time()\n",
    "\n",
    "    output, _ = process.communicate()\n",
    "    acc = float(output.decode().strip())\n",
    "    dur = end_time - start_time\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    d = {\"Accuracy\": acc, \"Time\": dur, \"Model size\": model_size, \"Memory usage\": mems}\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def run_python(dir_path, model_path):\n",
    "    '''Run Python inference and return dictionary with accuracy, duration, model size, and memory usage.'''\n",
    "    command = [\"python\", \"run.py\", model_path, dir_path]\n",
    "    mems = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    proc = psutil.Process(process.pid)\n",
    "    while process.poll() == None:\n",
    "        try:\n",
    "            # `proc.memory_info().rss` returns the physical memory the process has used\n",
    "            mems.append(proc.memory_info().rss / (1024 * 1024)) # append in megabytes\n",
    "            time.sleep(0.1)  # check memory usage every 0.1 second\n",
    "        except psutil.NoSuchProcess: # handle the case where the process ends abruptly\n",
    "            pass\n",
    "    end_time = time.time()\n",
    "\n",
    "    output, _ = process.communicate()\n",
    "    acc = float(output.decode().strip())\n",
    "    dur = end_time - start_time\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    d = {\"Accuracy\": acc, \"Time\": dur, \"Model size\": model_size, \"Memory usage\": mems}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46f98d-4ba2-4303-ad1f-b936484190d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Generate model files for tinyRuntime\n",
    "path = \"data\"\n",
    "model = load(\"resnet18\").model\n",
    "export_model(model, \"model.bin\")\n",
    "\n",
    "dls = ImageDataLoaders.from_folder(untar_data(URLs.IMAGENETTE_320), valid='val', item_tfms=Resize(224),\n",
    "                                   batch_tfms=Normalize.from_stats(*imagenet_stats), bs=64)\n",
    "model_prepared, qmodel = Quantizer().quantize_one_batch(model, dls.one_batch()[0])\n",
    "export_model_sq8(qmodel, model_prepared, \"model-q8.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64588404-2028-4b46-b1e1-1886f4e52fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: rerunning decrease memory usage in notebook, Python possible use previously allocated memory\n",
    "res0 = run_python(path, \"model.pkl\")\n",
    "res0[\"Accuracy\"], res0[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae643b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = run_c(path, \"./run\", \"model.bin\")\n",
    "res1[\"Accuracy\"], res1[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256adb54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res2 = run_c(path, \"./runq\", \"model-q8.bin\")\n",
    "res2[\"Accuracy\"], res2[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee9188-efd8-416d-8b73-c0f87f7a1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 4))\n",
    "ax[0].plot(res0[\"Memory usage\"])\n",
    "ax[1].plot(res1[\"Memory usage\"])\n",
    "ax[2].plot(res2[\"Memory usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def compare_results(res, architecture, runtime, quantized):\n",
    "    '''Compare the results and fail if performance is worse compared to the previous result'''\n",
    "    df = pd.read_csv(\"benchmark.csv\")\n",
    "    df = df[(df[\"Architecture\"] == architecture) & (df[\"Runtime\"] == runtime) & (df[\"Quantization\"] == quantized)]\n",
    "    if res[\"Accuracy\"] < 0.9 * df[\"Accuracy\"].values[-1]:\n",
    "        raise ValueError(f\"{runtime} - {quantized}: Accuracy is worse than 10%. Before: {df['Accuracy'].values[-1]}, Now: {res['Accuracy']}\")\n",
    "    if res[\"Time\"] > 1.25 * df[\"Time\"].values[-1]:\n",
    "        raise ValueError(f\"{runtime} - {quantized}: Time is worse than 25%. Before: {df['Time'].values[-1]}, Now: {res['Time']}\")\n",
    "\n",
    "def save_benchmark_csv():\n",
    "    # Get results\n",
    "    commit_id = os.getenv('GITHUB_SHA')\n",
    "    time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    architecture = platform.machine()\n",
    "    res0 = run_python(path, \"model.pkl\")\n",
    "    res1 = run_c(path, \"./run\", \"model.bin\")\n",
    "    res2 = run_c(path, \"./runq\", \"model-q8.bin\")\n",
    "    # raise error if performance is worse than earlier\n",
    "    compare_results(res0, architecture, \"PyTorch\", False)\n",
    "    compare_results(res1, architecture, \"tinyRuntime\", False)\n",
    "    compare_results(res2, architecture, \"tinyRuntime\", True)\n",
    "\n",
    "    def generate_dict(res, runtime, quantization=False):\n",
    "        d = {\"Commit\": commit_id, \"Datetime\": time, \"Architecture\": architecture, \"Runtime\": runtime,\n",
    "             \"Quantization\": quantization, \"Accuracy\": res[\"Accuracy\"], \"Time\": res[\"Time\"],\n",
    "             \"Model size\": res[\"Model size\"], \"Max memory\": np.max(res[\"Memory usage\"])}\n",
    "        return d\n",
    "\n",
    "    data = [generate_dict(res0, \"PyTorch\"), generate_dict(res1, \"tinyRuntime\"),\n",
    "            generate_dict(res2, \"tinyRuntime\", quantization=True)]\n",
    "\n",
    "    # Write results\n",
    "    csv_file = \"benchmark.csv\"\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'Data has been written to {csv_file}.')\n",
    "\n",
    "save_benchmark_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ce1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
